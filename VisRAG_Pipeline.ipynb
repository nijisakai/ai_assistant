{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nijisakai/ai_assistant/blob/main/VisRAG_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Libraries\n"
      ],
      "metadata": {
        "id": "reMwYvIn_0yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.40.2 sentencepiece==0.1.99 decord==0.6.0"
      ],
      "metadata": {
        "id": "OeDZS69E__-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Weighted Mean Pooling\n",
        "In this block, we define the `weighted_mean_pooling` function, which calculates `weighted mean pooling` on the modelâ€™s `hidden states`.\n"
      ],
      "metadata": {
        "id": "nxcKG1WL9su9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def weighted_mean_pooling(hidden, attention_mask):\n",
        "    # Apply cumulative sum to the attention mask to compute weighted pooling\n",
        "    attention_mask_ = attention_mask * attention_mask.cumsum(dim=1)\n",
        "\n",
        "    # Compute the sum of hidden states weighted by attention and then normalize\n",
        "    s = torch.sum(hidden * attention_mask_.unsqueeze(-1).float(), dim=1)\n",
        "    d = attention_mask_.sum(dim=1, keepdim=True).float()\n",
        "    reps = s / d\n",
        "    return reps"
      ],
      "metadata": {
        "id": "CVu2D67Y90vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Encoding Function\n",
        "Here, we define `encode`, a function that can handle both text and image inputs to generate embeddings.\n"
      ],
      "metadata": {
        "id": "B9QWTII396Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode(text_or_image_list):\n",
        "    # Check if input is text or image, then prepare inputs accordingly\n",
        "    if isinstance(text_or_image_list[0], str):\n",
        "        inputs = {\n",
        "            \"text\": text_or_image_list,\n",
        "            'image': [None] * len(text_or_image_list),\n",
        "            'tokenizer': tokenizer\n",
        "        }\n",
        "    else:\n",
        "        inputs = {\n",
        "            \"text\": [''] * len(text_or_image_list),\n",
        "            'image': text_or_image_list,\n",
        "            'tokenizer': tokenizer\n",
        "        }\n",
        "\n",
        "    # Forward pass through the model\n",
        "    outputs = model(**inputs)\n",
        "    attention_mask = outputs.attention_mask\n",
        "    hidden = outputs.last_hidden_state\n",
        "\n",
        "    # Apply weighted mean pooling and normalize the result\n",
        "    reps = weighted_mean_pooling(hidden, attention_mask)\n",
        "    embeddings = F.normalize(reps, p=2, dim=1).detach().cpu().numpy()\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "BtcTXm8j-A2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load VisRAG-Ret\n",
        "This block loads the `VisRAG-Ret` model and tokenizer from Hugging Face. We also specify `torch.float16` as the data type for compatibility with T4 GPUs."
      ],
      "metadata": {
        "id": "zVo9ujLu-eyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model and Tokenizer\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load the VisRAG-Ret model and tokenizer\n",
        "model_name_or_path = \"openbmb/VisRAG-Ret\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
        "# Since the T4 GPU doesn't support torch.bfloat16, we use torch.float16 instead.\n",
        "model = AutoModel.from_pretrained(model_name_or_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "x2iWy5_W-mMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Input Query and Download Test Images\n",
        "This block defines sample queries and downloads test images for evaluating the model's capability to match queries with relevant images.\n"
      ],
      "metadata": {
        "id": "pJ1Ap7v8-s0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Define sample query\n",
        "queries = [\"What does a dog look like?\"]\n",
        "INSTRUCTION = \"Represent this query for retrieving relevant documents: \"\n",
        "queries = [INSTRUCTION + query for query in queries]\n",
        "\n",
        "# Download sample images\n",
        "print(\"Downloading images...\")\n",
        "passages = [\n",
        "    Image.open(BytesIO(requests.get(\n",
        "        'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/cat.jpeg'\n",
        "    ).content)).convert('RGB'),\n",
        "    Image.open(BytesIO(requests.get(\n",
        "        'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/dog.jpg'\n",
        "    ).content)).convert('RGB')\n",
        "]\n",
        "print(\"Images downloaded.\")\n"
      ],
      "metadata": {
        "id": "NNDEn4bv_GOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Embeddings and Calculate Similarity Scores\n",
        "In this section, we encode the queries and images, then compute similarity scores between the query embedding and each image embedding.\n"
      ],
      "metadata": {
        "id": "viI1RQ97_JCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the queries and images to get embeddings\n",
        "embeddings_query = encode(queries)\n",
        "embeddings_doc = encode(passages)\n",
        "\n",
        "# Calculate similarity scores\n",
        "scores = (embeddings_query @ embeddings_doc.T)\n",
        "print(\"Similarity scores:\", scores.tolist())  # [[0.25753140449523926, 0.3385779857635498]], higher score for the dog image\n"
      ],
      "metadata": {
        "id": "EsWkjDxF_OhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use VisRAG-Gen for Generation with Image\n",
        "Finally, we use the `MiniCPM-V-2` model to generate a response based on the image that best matches the query.\n"
      ],
      "metadata": {
        "id": "MM-EFGpT_RBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VisRAG-Gen model and tokenizer for generation\n",
        "# Since the T4 GPU doesn't support torch.bfloat16, we use torch.float16 instead.\n",
        "model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True, torch_dtype=torch.float16).to(device='cuda', dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True)\n",
        "model.eval()\n",
        "\n",
        "# Choose the best matching image (dog) based on similarity scores: [[0.25753140449523926, 0.3385779857635498]]\n",
        "image = passages[1]  # The image representing a dog\n",
        "msgs = [{'role': 'user', 'content': queries[0]}]\n",
        "\n",
        "# Generate response based on the query and image\n",
        "res, context, _ = model.chat(\n",
        "    image=image,\n",
        "    msgs=msgs,\n",
        "    context=None,\n",
        "    tokenizer=tokenizer,\n",
        "    sampling=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "print(\"Generated response:\", res)"
      ],
      "metadata": {
        "id": "H9uaVH2O_V28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}